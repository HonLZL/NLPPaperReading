# Paper reading list for NLP research

## Transformer architecture

- Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. **Attention is all you need**. In *Advances in neural information processing systems*, volume 30. Curran Associates, Inc. 2017. [pdf](https://doi.org/10/gpnmtv)

## Pre-trained Models

- Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. **BERT: pre-training of deep bidirectional transformers for language understanding**. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. 2019. [pdf](https://doi.org/10/ggbwf6)

- Mike Lewis, Yinhan Liu, Naman Goyal, et al. **BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension**. In *Proceedings of the 58th annual meeting of the association for computational linguistics*, pages 7871–7880, Online. Association for Computational Linguistics. 2020. [pdf](https://doi.org/10.18653/v1/2020.acl-main.703)

- Alec Radford, Tim Salimans, and Ilya Sutskever. **Improving language understanding by generative pre-training**.In 2018. [pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

## QA

For **any** questions, feel free to open issues or contact [zhuwenq](https://github.com/Leonezz) directly.