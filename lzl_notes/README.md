# Paper reading list for NLP research

## Transformer architecture

- **Attention is all you need**. *Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, et al,. *In* NIPS 2017. [pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

## Pre-trained Models

- **BERT: pre-training of deep bidirectional transformers for language understanding**. *Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, *Kristina Toutanova*. *In* NAACL 2019. [pdf](https://aclanthology.org/N19-1423.pdf)
- **BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension**. *Mike Lewis*, *Yinhan Liu*, *Naman Goyal*, et al,. *In* ACL 2020. [pdf](https://aclanthology.org/2020.acl-main.703.pdf)
- **Improving language understanding by generative pre-training**. *Alec Radford*, *Tim Salimans*, *Ilya Sutskever*. *In* OpenAI blog 2018. [pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)


## QA

For **any** questions, feel free to open issues or contact [zhuwenq](https://github.com/Leonezz) directly.