# Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context

## 1 背景



Transformers 虽然具有学习长期依赖关系的潜力，但在语言建模的设置上受到固定长度上下文的限制。比如原 transformer限制为 512 ，

如何学习长期一代一直是一个长期存在的研究问题，以往都是修改 RNN 的内部架构去化解梯度消失。然而本文基于 transformer。 





## 2 模型

