# Parameter-Efficient Transfer Learning for NLP

## 1 背景

微调预训练大模型是低效的，每个任务都需要一个全新的模型，这是不划算的。

仅仅微调顶层参数是不够的。



NLP中最常见的两种迁移学习技术是基于特征的迁移（feature-based transfer）和微调 （fine-tuning）。

基于特征是指文本经过预训练模型后输出特征，该特征作为其他模型的输入。类似特征提取。

> 二者之间最主要的区别在于预训练模型的参数是否会随着新数据的加入而进行调整。基于特征的迁移不会，微调会改变原始预训练模型的参数。有关文章说明微调是更有效的。

$x$ 是输入向量，预训练模型的参数是 $w$，

预训练模型：$\phi_w(x)$ 

基于特征的迁移，相当于把预训练输出的内容再经过一个模型，设新函数为 $\chi_v (x)$，$v$ 是下游模型的参数，所以全过程可以用$ \chi_v (\phi_w(x))$ 来表示

基于微调的迁移：只去调整 $w$ 到 $w_0$，即最终表达式为 $\phi_{w_0}(x)$ 

基于 Adapter 的迁移：$\psi_{w,v}(x)$，$w$ 是预训练模型的参数，初始化这个 $\phi$ 时，$\psi_{w,v_0}(x)\approx \phi_w(x)$，在下游训练阶段，只有 $v$ 被微调。如果新增的参数 $x$，的参数量远小于预训练的参数 $w$ 的参数即，即： $|v| \ll |w|$，那么这个基于 Adapter 的预训练将适合许多个模型，因为 $w$ 被冻结了，微调时不会改变预训练的参数。































